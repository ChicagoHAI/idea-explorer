{"title": "Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance", "year": 2024, "authors": "Ziqi Yin, Hao Wang, Kaito Horio, Daisuke Kawahara, Satoshi Sekine", "url": "https://www.semanticscholar.org/paper/eadccfc70b5a7898b7fb4bad04e668d93a2edf6b", "relevance": 3, "abstract": "We investigate the impact of politeness levels in prompts on the performance of large language models (LLMs). Polite language in human communications often garners more compliance and effectiveness, while rudeness can cause aversion, impacting response quality. We consider that LLMs mirror human communication traits, suggesting they align with human cultural norms. We assess the impact of politeness in prompts on LLMs across English, Chinese, and Japanese tasks. We observed that impolite prompts often result in poor performance, but overly polite language does not guarantee better outcomes. The best politeness level is different according to the language. This phenomenon suggests that LLMs not only reflect human behavior but are also influenced by language, particularly in different cultural contexts. Our findings highlight the need to factor in politeness for cross-cultural natural language processing and LLM usage.", "citations": 51}
{"title": "Visual Instruction Tuning with Polite Flamingo", "year": 2023, "authors": "Delong Chen, Jianfeng Liu, Wenliang Dai, Baoyuan Wang", "url": "https://api.semanticscholar.org/CorpusId:259316495", "relevance": 3, "abstract": "Recent research has demonstrated that the multi-task fine-tuning of multi-modal Large Language Models (LLMs) using an assortment of annotated downstream vision-language datasets significantly enhances their performance. Yet, during this process, a side effect, which we termed as the \"multi-modal alignment tax\", surfaces. This side effect negatively impacts the model's ability to format responses appropriately - for instance, its \"politeness\" - due to the overly succinct and unformatted nature of raw annotations, resulting in reduced human preference. In this paper, we introduce Polite Flamingo, a multi-modal response rewriter that transforms raw annotations into a more appealing, \"polite\" format. Polite Flamingo is trained to reconstruct high-quality responses from their automatically distorted counterparts and is subsequently applied to a vast array of vision-language datasets for response rewriting. After rigorous filtering, we generate the PF-1M dataset and further validate its value by fine-tuning a multi-modal LLM with it. Combined with novel methodologies including U-shaped multi-stage tuning and multi-turn augmentation, the resulting model, Clever Flamingo, demonstrates its advantages in both multi-modal understanding and response politeness according to automated and human evaluations. Code and dataset are available at https://github.com/ChenDelong1999/polite-flamingo", "citations": 53}
{"title": "Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA", "year": 2025, "authors": "Hanyu Cai, Binqi Shen, Lier Jin, Lan Hu, Xiaojing Fan", "url": "https://www.semanticscholar.org/paper/e0d5a840d7e4a5b385e9bf1de72abb9aeeb5e5df", "relevance": 3, "abstract": "Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing. Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.", "citations": 4}
{"title": "Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)", "year": 2025, "authors": "Om Dobariya, Akhil Kumar", "url": "https://api.semanticscholar.org/CorpusId:281843710", "relevance": 3, "abstract": "The wording of natural language prompts has been shown to influence the performance of large language models (LLMs), yet the role of politeness and tone remains underexplored. In this study, we investigate how varying levels of prompt politeness affect model accuracy on multiple-choice questions. We created a dataset of 50 base questions spanning mathematics, science, and history, each rewritten into five tone variants: Very Polite, Polite, Neutral, Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we evaluated responses across these conditions and applied paired sample t-tests to assess statistical significance. Contrary to expectations, impolite prompts consistently outperformed polite ones, with accuracy ranging from 80.8% for Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from earlier studies that associated rudeness with poorer outcomes, suggesting that newer LLMs may respond differently to tonal variation. Our results highlight the importance of studying pragmatic aspects of prompting and raise broader questions about the social dimensions of human-AI interaction.", "citations": 3}
{"title": "Cost Transparency of Enterprise AI Adoption", "year": 2025, "authors": "Soogand Alavi, Salar Nozari, Andrea Luangrath", "url": "https://api.semanticscholar.org/CorpusId:283073523", "relevance": 3, "abstract": "Recent advances in large language models (LLMs) have dramatically improved performance on a wide range of tasks, driving rapid enterprise adoption. Yet, the cost of adopting these AI services is understudied. Unlike traditional software licensing in which costs are predictable before usage, commercial LLM services charge per token of input text in addition to generated output tokens. Crucially, while firms can control the input, they have limited control over output tokens, which are effectively set by generation dynamics outside of business control. This research shows that subtle shifts in linguistic style can systematically alter the number of output tokens without impacting response quality. Using an experiment with OpenAI's API, this study reveals that non-polite prompts significantly increase output tokens leading to higher enterprise costs and additional revenue for OpenAI. Politeness is merely one instance of a broader phenomenon in which linguistic structure can drive unpredictable cost variation. For enterprises integrating LLM into applications, this unpredictability complicates budgeting and undermines transparency in business-to-business contexts. By demonstrating how end-user behavior links to enterprise costs through output token counts, this work highlights the opacity of current pricing models and calls for new approaches to ensure predictable and transparent adoption of LLM services.", "citations": 0}
{"title": "Beyond Demographics: Fine-tuning Large Language Models to Predict Individuals' Subjective Text Perceptions", "year": 2025, "authors": "Matthias Orlikowski, Jiaxin Pei, Paul Rottger, Philipp Cimiano, David Jurgens, Dirk Hovy", "url": "https://api.semanticscholar.org/CorpusId:276724912", "relevance": 3, "abstract": "People naturally vary in their annotations for subjective questions and some of this variation is thought to be due to the person's sociodemographic characteristics. LLMs have also been used to label data, but recent work has shown that models perform poorly when prompted with sociodemographic attributes, suggesting limited inherent sociodemographic knowledge. Here, we ask whether LLMs can be trained to be accurate sociodemographic models of annotator variation. Using a curated dataset of five tasks with standardized sociodemographics, we show that models do improve in sociodemographic prompting when trained but that this performance gain is largely due to models learning annotator-specific behaviour rather than sociodemographic patterns. Across all tasks, our results suggest that models learn little meaningful connection between sociodemographics and annotation, raising doubts about the current use of LLMs for simulating sociodemographic variation and behaviour.", "citations": 23}
{"title": "Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models", "year": 2024, "authors": "R. Vinay, G. Spitale, N. Biller-Andorno, Federico Germani", "url": "https://api.semanticscholar.org/CorpusId:268253378", "relevance": 3, "abstract": "This study investigates the generation of synthetic disinformation by OpenAI's Large Language Models (LLMs) through prompt engineering and explores their responsiveness to emotional prompting. Leveraging various LLM iterations using davinci-002, davinci-003, gpt-3.5-turbo and gpt-4, we designed experiments to assess their success in producing disinformation. Our findings, based on a corpus of 19,800 synthetic disinformation social media posts, reveal that all LLMs by OpenAI can successfully produce disinformation, and that they effectively respond to emotional prompting, indicating their nuanced understanding of emotional cues in text generation. When prompted politely, all examined LLMs consistently generate disinformation at a high frequency. Conversely, when prompted impolitely, the frequency of disinformation production diminishes, as the models often refuse to generate disinformation and instead caution users that the tool is not intended for such purposes. This research contributes to the ongoing discourse surrounding responsible development and application of AI technologies, particularly in mitigating the spread of disinformation and promoting transparency in AI-generated content.", "citations": 8}
{"title": "Cooking Up Politeness in Human-AI Information Seeking Dialogue", "year": 2026, "authors": "David Elsweiler, Christine Elsweiler, Anna Ziegner", "url": "https://api.semanticscholar.org/CorpusId:284738388", "relevance": 3, "abstract": "Politeness is a core dimension of human communication, yet its role in human-AI information seeking remains underexplored. We investigate how user politeness behaviour shapes conversational outcomes in a cooking-assistance setting. First, we annotated 30 dialogues, identifying four distinct user clusters ranging from Hyperpolite to Hyperefficient. We then scaled up to 18,000 simulated conversations across five politeness profiles (including impolite) and three open-weight models. Results show that politeness is not only cosmetic: it systematically affects response length, informational gain, and efficiency. Engagement-seeking prompts produced up to 90% longer replies and 38% more information nuggets than hyper-efficient prompts, but at markedly lower density. Impolite inputs yielded verbose but less efficient answers, with up to 48% fewer nuggets per watt-hour compared to polite input. These findings highlight politeness as both a fairness and sustainability issue: conversational styles can advantage or disadvantage users, and\"polite\"requests may carry hidden energy costs. We discuss implications for inclusive and resource-aware design of information agents.", "citations": 0}
{"title": "Human\u2013computer pragmatics trialled: some (im)polite interactions with ChatGPT 4.0 and the ensuing implications", "year": 2024, "authors": "Zhi Quan, Zhiwei Chen", "url": "https://www.semanticscholar.org/paper/1a2c5a2328be7d1bbdc8c1f1255f627ccd34df2c", "relevance": 3, "abstract": "ABSTRACT Upon rapid evolution, ChatGPT can now generate content that is linguistically accurate and logically sound, while sidestepping ethical, social and legal concerns. This research seeks to investigate whether ChatGPT will employ different pragmatic strategies in its responses to (im)polite questions. In our experiment, this AI-powered tool was instructed to answer 200 self-made questions over four (im)politeness levels, and the 200 responses were collected to go through linguistic and sentiment analysis. Triangulated data, together with typical examples, show that ChatGPT tends to give shorter and less positive answers to less polite questions, appearing to be less responsive when confronted with more blunt and offensive inquiries. This, to some extent, resembles how human beings react when treated impolitely. A tentative explanation may be that, given its nature as a large language model, ChatGPT mirrors human interaction in various scenarios, and draws on prevalent human communication tendencies. Thus, interacting with ChatGPT is more of a human-society interaction than human-machine communication in the real sense. Our research sheds light on the coined \u201chuman-machine pragmatics\u201d, i.e. how humans can best communicate with computers for the best informative and affective outcomes. The implications for language education are also discussed in the end.", "citations": 5}
{"title": "Using cognitive models to reveal value trade-offs in language models", "year": 2025, "authors": "Sonia K. Murthy, Rosie Zhao, Jennifer Hu, S. Kakade, Markus Wulfmeier, Peng Qian, Tomer D. Ullman", "url": "https://api.semanticscholar.org/CorpusId:280012067", "relevance": 3, "abstract": "Value trade-offs are an integral part of human decision-making and language use, however, current tools for interpreting such dynamic and multi-faceted notions of values in LLMs are limited. In cognitive science, so-called\"cognitive models\"provide formal accounts of such trade-offs in humans, by modeling the weighting of a speaker's competing utility functions in choosing an action or utterance. Here we use a leading cognitive model of polite speech to systematically evaluate value trade-offs in two encompassing model settings: degrees of reasoning\"effort\"in frontier black-box models, and RL post-training dynamics of open-source models. Our results highlight patterns of higher informational utility than social utility in reasoning models'default behavior, and demonstrate that these patterns shift in predictable ways when models are prompted to prioritize certain goals over others. Our findings from LLMs'training dynamics suggest large shifts in utility values early on in training with persistent effects of the choice of base model and pretraining data, compared to feedback dataset or alignment method. Our framework offers a flexible tool for probing value trade-offs across diverse model types, providing insights for generating hypotheses about other social behaviors such as sycophancy and for shaping training regimes that better control trade-offs between values during model development.", "citations": 0}
{"title": "Polite Dialogue Generation Without Parallel Data", "year": 2018, "authors": "Tong Niu, Mohit Bansal", "url": "https://www.semanticscholar.org/paper/f1cba8a5a73c8151c2f5cb6edd5bc6a7c03e80fa", "relevance": 2, "abstract": "Stylistic dialogue response generation, with valuable applications in personality-based conversational agents, is a challenging task because the response needs to be fluent, contextually-relevant, as well as paralinguistically accurate. Moreover, parallel datasets for regular-to-stylistic pairs are usually unavailable. We present three weakly-supervised models that can generate diverse, polite (or rude) dialogue responses without parallel data. Our late fusion model (Fusion) merges the decoder of an encoder-attention-decoder dialogue model with a language model trained on stand-alone polite utterances. Our label-finetuning (LFT) model prepends to each source sequence a politeness-score scaled label (predicted by our state-of-the-art politeness classifier) during training, and at test time is able to generate polite, neutral, and rude responses by simply scaling the label embedding by the corresponding score. Our reinforcement learning model (Polite-RL) encourages politeness generation by assigning rewards proportional to the politeness classifier score of the sampled response. We also present two retrievalbased, polite dialogue model baselines. Human evaluation validates that while the Fusion and the retrieval-based models achieve politeness with poorer context-relevance, the LFT and Polite-RL models can produce significantly more polite responses without sacrificing dialogue quality.", "citations": 178}
{"title": "Comparing human and LLM politeness strategies in free production", "year": 2025, "authors": "Haoran Zhao, Robert D. Hawkins", "url": "https://api.semanticscholar.org/CorpusId:279305933", "relevance": 2, "abstract": "Polite speech poses a fundamental alignment challenge for large language models (LLMs). Humans deploy a rich repertoire of linguistic strategies to balance informational and social goals -- from positive approaches that build rapport (compliments, expressions of interest) to negative strategies that minimize imposition (hedging, indirectness). We investigate whether LLMs employ a similarly context-sensitive repertoire by comparing human and LLM responses in both constrained and open-ended production tasks. We find that larger models ($\\ge$70B parameters) successfully replicate key preferences from the computational pragmatics literature, and human evaluators surprisingly prefer LLM-generated responses in open-ended contexts. However, further linguistic analyses reveal that models disproportionately rely on negative politeness strategies even in positive contexts, potentially leading to misinterpretations. While modern LLMs demonstrate an impressive handle on politeness strategies, these subtle differences raise important questions about pragmatic alignment in AI systems.", "citations": 4}
{"title": "No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models", "year": 2025, "authors": "F. Plaza-del-Arco, Paul R\u00f6ttger, Nino Scherrer, Emanuele Borgonovo, E. Plischke, Dirk Hovy", "url": "https://api.semanticscholar.org/CorpusId:281244007", "relevance": 2, "abstract": "Large language models (LLMs) are increasingly integrated into our daily lives and personalized. However, LLM personalization might also increase unintended side effects. Recent work suggests that persona prompting can lead models to falsely refuse user requests. However, no work has fully quantified the extent of this issue. To address this gap, we measure the impact of 15 sociodemographic personas (based on gender, race, religion, and disability) on false refusal. To control for other factors, we also test 16 different models, 3 tasks (Natural Language Inference, politeness, and offensiveness classification), and nine prompt paraphrases. We propose a Monte Carlo-based method to quantify this issue in a sample-efficient manner. Our results show that as models become more capable, personas impact the refusal rate less and less. Certain sociodemographic personas increase false refusal in some models, which suggests underlying biases in the alignment strategies or safety mechanisms. However, we find that the model choice and task significantly influence false refusals, especially in sensitive content tasks. Our findings suggest that persona effects have been overestimated, and might be due to other factors.", "citations": 1}
{"title": "TyDiP: A Dataset for Politeness Classification in Nine Typologically Diverse Languages", "year": 2022, "authors": "A. Srinivasan, Eunsol Choi", "url": "https://api.semanticscholar.org/CorpusId:254069965", "relevance": 2, "abstract": "We study politeness phenomena in nine typologically diverse languages. Politeness is an important facet of communication and is sometimes argued to be cultural-specific, yet existing computational linguistic study is limited to English. We create TyDiP, a dataset containing three-way politeness annotations for 500 examples in each language, totaling 4.5K examples. We evaluate how well multilingual models can identify politeness levels -- they show a fairly robust zero-shot transfer ability, yet fall short of estimated human accuracy significantly. We further study mapping the English politeness strategy lexicon into nine languages via automatic translation and lexicon induction, analyzing whether each strategy's impact stays consistent across languages. Lastly, we empirically study the complicated relationship between formality and politeness through transfer experiments. We hope our dataset will support various research questions and applications, from evaluating multilingual models to constructing polite multilingual agents.", "citations": 22}
{"title": "Sociodemographic Prompting is Not Yet an Effective Approach for Simulating Subjective Judgments with LLMs", "year": 2023, "authors": "Huaman Sun, Jiaxin Pei, Minje Choi, David Jurgens", "url": "https://api.semanticscholar.org/CorpusId:265221055", "relevance": 2, "abstract": "Human judgments are inherently subjective and are actively affected by personal traits such as gender and ethnicity. While Large Language Models (LLMs) are widely used to simulate human responses across diverse contexts, their ability to account for demographic differences in subjective tasks remains uncertain. In this study, leveraging the POPQUORN dataset, we evaluate nine popular LLMs on their ability to understand demographic differences in two subjective judgment tasks: politeness and offensiveness. We find that in zero-shot settings, most models' predictions for both tasks align more closely with labels from White participants than those from Asian or Black participants, while only a minor gender bias favoring women appears in the politeness task. Furthermore, sociodemographic prompting does not consistently improve and, in some cases, worsens LLMs' ability to perceive language from specific sub-populations. These findings highlight potential demographic biases in LLMs when performing subjective judgment tasks and underscore the limitations of sociodemographic prompting as a strategy to achieve pluralistic alignment. Code and data are available at: https://github.com/Jiaxin-Pei/LLM-as-Subjective-Judge.", "citations": 26}
{"title": "Out of Style: RAG's Fragility to Linguistic Variation", "year": 2025, "authors": "Tianyu Cao, Neel Bhandari, Akhila Yerukola, Akari Asai, Maarten Sap", "url": "https://api.semanticscholar.org/CorpusId:277741211", "relevance": 2, "abstract": "Despite the impressive performance of Retrieval-augmented Generation (RAG) systems across various NLP benchmarks, their robustness in handling real-world user-LLM interaction queries remains largely underexplored. This presents a critical gap for practical deployment, where user queries exhibit greater linguistic variations and can trigger cascading errors across interdependent RAG components. In this work, we systematically analyze how varying four linguistic dimensions (formality, readability, politeness, and grammatical correctness) impact RAG performance. We evaluate two retrieval models and nine LLMs, ranging from 3 to 72 billion parameters, across four information-seeking Question Answering (QA) datasets. Our results reveal that linguistic reformulations significantly impact both retrieval and generation stages, leading to a relative performance drop of up to 40.41% in Recall@5 scores for less formal queries and 38.86% in answer match scores for queries containing grammatical errors. Notably, RAG systems exhibit greater sensitivity to such variations compared to LLM-only generations, highlighting their vulnerability to error propagation due to linguistic shifts. These findings highlight the need for improved robustness techniques to enhance reliability in diverse user interactions. Code is available at https://github.com/Springcty/RAG-fragility-to-linguistic-variation.", "citations": 6}
{"title": "SADA: Safe and Adaptive Aggregation of Multiple Black-Box Predictions in Semi-Supervised Learning", "year": 2025, "authors": "Jiawei Shan, Zhifeng Chen, Yiming Dong, Yazhen Wang, Jiwei Zhao", "url": "https://api.semanticscholar.org/CorpusId:281658875", "relevance": 2, "abstract": "Semi-supervised learning (SSL) arises in practice when labeled data are scarce or expensive to obtain, while large quantities of unlabeled data are readily available. With the growing adoption of machine learning techniques, it has become increasingly feasible to generate multiple predicted labels using a variety of models and algorithms, including deep learning, large language models, and generative AI. In this paper, we propose a novel approach that safely and adaptively aggregates multiple black-box predictions of uncertain quality for both inference and prediction tasks. Our method provides two key guarantees: (i) it never performs worse than using the labeled data alone, regardless of the quality of the predictions; and (ii) if any one of the predictions (without knowing which one) perfectly fits the ground truth, the algorithm adaptively exploits this to achieve either a faster convergence rate or the semiparametric efficiency bound. We demonstrate the effectiveness of the proposed algorithm through small-scale simulations and two real-data analyses with distinct scientific goals. A user-friendly R package, sada, is provided to facilitate practical implementation.", "citations": 0}
{"title": "Can ChatGPT recognize impoliteness? An exploratory study of the pragmatic awareness of a large language model", "year": 2025, "authors": "Marta Andersson, Dan McIntyre", "url": "https://www.semanticscholar.org/paper/e6a2d06fe5574ab923abda2a76645b022406dd8a", "relevance": 2, "abstract": "", "citations": 10}
{"title": "Which Demographics do LLMs Default to During Annotation?", "year": 2024, "authors": "Johannes Sch\u00e4fer, Aidan Combs, Christopher Bagdon, Jiahui Li, Nadine Probol, Lynn Greschner, Sean Papay, Yarik Menchaca Resendiz, Aswathy Velutharambath, Amelie W\u00fchrl, Sabine Weber, Roman Klinger", "url": "https://api.semanticscholar.org/CorpusId:273323307", "relevance": 2, "abstract": "Demographics and cultural background of annotators influence the labels they assign in text annotation -- for instance, an elderly woman might find it offensive to read a message addressed to a\"bro\", but a male teenager might find it appropriate. It is therefore important to acknowledge label variations to not under-represent members of a society. Two research directions developed out of this observation in the context of using large language models (LLM) for data annotations, namely (1) studying biases and inherent knowledge of LLMs and (2) injecting diversity in the output by manipulating the prompt with demographic information. We combine these two strands of research and ask the question to which demographics an LLM resorts to when no demographics is given. To answer this question, we evaluate which attributes of human annotators LLMs inherently mimic. Furthermore, we compare non-demographic conditioned prompts and placebo-conditioned prompts (e.g.,\"you are an annotator who lives in house number 5\") to demographics-conditioned prompts (\"You are a 45 year old man and an expert on politeness annotation. How do you rate {instance}\"). We study these questions for politeness and offensiveness annotations on the POPQUORN data set, a corpus created in a controlled manner to investigate human label variations based on demographics which has not been used for LLM-based analyses so far. We observe notable influences related to gender, race, and age in demographic prompting, which contrasts with previous studies that found no such effects.", "citations": 8}
{"title": "Steering Language Models Before They Speak: Logit-Level Interventions", "year": 2026, "authors": "Hyeseon An, Shinwoo Park, Hyundong Jin, Yo-Sub Han", "url": "https://api.semanticscholar.org/CorpusId:284860613", "relevance": 2, "abstract": "Steering LLMs is essential for specialized applications such as style-sensitive text rewriting, user-adaptive communication, and toxicity mitigation. Current steering methods, such as prompting-based and activation-based approaches, are widely used to guide model behavior. However, activation-based techniques require deep access to internal layers, while prompting-based steering often fails to provide consistent or fine-grained control. In order to address these limitations, we propose a training-free inference-time logit intervention for controllable generation. Our approach utilizes a statistical token score table derived from z-normalized log-odds of labeled corpora to shift the decoding distribution. Empirical evaluations across three diverse datasets focusing on writing complexity, formality, and toxicity demonstrate that our method effectively steers output characteristics, confirming its broad applicability and task-agnostic nature. Our results show that statistically grounded logit steering can achieve large, consistent, and multi-task control gains: up to +47%p accuracy and 50x f1 improvement.", "citations": 0}
{"title": "Transformer Architecture-Based Transfer Learning for Politeness Prediction in Conversation", "year": 2023, "authors": "Shakir Khan, Mohd Fazil, A. Imoize, Bayan Alabduallah, Bader M. Albahlal, Saad Abdullah Alajlan, Abrar Almjally, Tamanna Siddiqui", "url": "https://api.semanticscholar.org/CorpusId:259850579", "relevance": 1, "abstract": "Politeness is an essential part of a conversation. Like verbal communication, politeness in textual conversation and social media posts is also stimulating. Therefore, the automatic detection of politeness is a significant and relevant problem. The existing literature generally employs classical machine learning-based models like naive Bayes and Support Vector-based trained models for politeness prediction. This paper exploits the state-of-the-art (SOTA) transformer architecture and transfer learning for respectability prediction. The proposed model employs the strengths of context-incorporating large language models, a feed-forward neural network, and an attention mechanism for representation learning of natural language requests. The trained representation is further classified using a softmax function into polite, impolite, and neutral classes. We evaluate the presented model employing two SOTA pre-trained large language models on two benchmark datasets. Our model outperformed the two SOTA and six baseline models, including two domain-specific transformer-based models using both the BERT and RoBERTa language models. The ablation investigation shows that the exclusion of the feed-forward layer displays the highest impact on the presented model. The analysis reveals the batch size and optimization algorithms as effective parameters affecting the model performance.", "citations": 13}
{"title": "How Well Can Language Models Understand Politeness?", "year": 2023, "authors": "Can Li, Bin Pang, Wenbo Wang, Lingshu Hu, Matthew J. Gordon, Detelina Marinova, Bitty Balducci, Yi Shang", "url": "https://www.semanticscholar.org/paper/97b3fc96d7f9352f174f0c2f71ecd7b1e30ef24e", "relevance": 1, "abstract": "Politeness plays a key role in social communications. Previous work proposed an SVM-based computational method for predicting politeness using linguistic features on a corpus that contains Wikipedia and Stack Exchange requests data. To extend this prior work, we focus on evaluating the performance of state-of-the-art language models on politeness prediction using the same dataset. Two models are applied in this study. First, we fine-tune BERT on politeness data and then use the fine-tuned model for politeness prediction. Second, we use ChatGPT to predict politeness. The results show that both fine-tuned BERT and ChatGPT achieved better results than the state-of-the-art results on both Wikipedia and Stack Exchange data. Fine-tuned BERT outperforms zero shot ChatGPT, but ChatGPT can provide explanations for its prediction. Moreover, fine-tuned BERT outperforms human-level performance by 2.28% on Wikipedia corpus.", "citations": 10}
{"title": "Are Current Task-Oriented Dialogue Systems Able to Satisfy Impolite Users?", "year": 2022, "authors": "Zhiqiang Hu, Roy Ka-Wei Lee, Nancy F. Chen", "url": "https://api.semanticscholar.org/CorpusId:253098716", "relevance": 1, "abstract": "Task-oriented dialogue (TOD) systems play a critical role in assisting users with various tasks, such as ticket booking and service inquiries. While these systems have demonstrated significant potential in addressing customer needs, they typically assume that users will interact with the dialogue agent in a polite manner. This assumption, however, is often unrealistic, as users may express impatience or frustration through impolite behavior. Addressing this gap, this article investigates the impact of impolite user behavior on the performance of TOD systems. To this end, we developed a novel corpus of impolite dialogues and conducted comprehensive experiments to evaluate the performance of state-of-the-art TOD systems on this dataset. Our results reveal a notable limitation: existing TOD systems struggle to handle impolite user utterances effectively, leading to degraded performance. To mitigate this issue, we introduce a data augmentation approach designed to improve the systems\u2019 ability to manage impolite dialogues. Although this method achieves measurable improvements, managing impolite user interactions remains a challenging research problem. By making our impolite dialogue corpus publicly accessible, we aim to encourage further research in this underexplored area. This study underscores the need for more robust TOD systems capable of handling diverse user behaviors, ultimately enhancing their applicability in real-world scenarios.", "citations": 5}
{"title": "Computational Politeness in Natural Language Processing: A Survey", "year": 2024, "authors": "Priyanshu Priya, Mauajama Firdaus, Asif Ekbal", "url": "https://api.semanticscholar.org/CorpusId:268910647", "relevance": 1, "abstract": "Computational approach to politeness is the task of automatically predicting and/or generating politeness in text. This is a pivotal task for conversational analysis, given the ubiquity and challenges of politeness in interactions. The computational approach to politeness has witnessed great interest from the conversational analysis community. This article is a compilation of past works in computational politeness in natural language processing. We view four milestones in the research so far, viz. supervised and weakly supervised feature extraction to identify and induce politeness in a given text, incorporation of context beyond the target text, study of politeness across different social factors, and study the relationship between politeness and various socio-linguistic cues. In this article, we describe the datasets, approaches, trends, and issues in computational politeness research. We also discuss representative performance values and provide pointers to future works, as given in the prior works. In terms of resources to understand the state of the art, this survey presents several valuable illustrations\u2014most prominently, a table summarizing the past papers along different dimensions, such as the types of features, annotation techniques, and datasets used.", "citations": 18}
{"title": "StyLEx: Explaining Style Using Human Lexical Annotations", "year": 2022, "authors": "Shirley Anugrah Hayati, Kyumin Park, Dheeraj Rajagopal, Lyle Ungar, Dongyeop Kang", "url": "https://api.semanticscholar.org/CorpusId:258170532", "relevance": 1, "abstract": "Large pre-trained language models have achieved impressive results on various style classification tasks, but they often learn spurious domain-specific words to make predictions (Hayati et al., 2021). While human explanation highlights stylistic tokens as important features for this task, we observe that model explanations often do not align with them. To tackle this issue, we introduce StyLEx, a model that learns from human annotated explanations of stylistic features and jointly learns to perform the task and predict these features as model explanations. Our experiments show that StyLEx can provide human like stylistic lexical explanations without sacrificing the performance of sentence-level style prediction on both in-domain and out-of-domain datasets. Explanations from StyLEx show significant improvements in explanation metrics (sufficiency, plausibility) and when evaluated with human annotations. They are also more understandable by human judges compared to the widely-used saliency-based explanation baseline.", "citations": 3}
{"title": "Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models", "year": 2023, "authors": "Victor Steinborn, Antonis Maronikolakis, Hinrich Sch\u00fctze", "url": "https://api.semanticscholar.org/CorpusId:259187663", "relevance": 1, "abstract": "In efforts to keep up with the rapid progress and use of large language models, gender bias research is becoming more prevalent in NLP. Non-English bias research, however, is still in its infancy with most work focusing on English. In our work, we study how grammatical gender bias relating to politeness levels manifests in Japanese and Korean language models. Linguistic studies in these languages have identified a connection between gender bias and politeness levels, however it is not yet known if language models reproduce these biases. We analyze relative prediction probabilities of the male and female grammatical genders using templates and find that informal polite speech is most indicative of the female grammatical gender, while rude and formal speech is most indicative of the male grammatical gender. Further, we find politeness levels to be an attack vector for allocational gender bias in cyberbullying detection models. Cyberbullies can evade detection through simple techniques abusing politeness levels. We introduce an attack dataset to (i) identify representational gender bias across politeness levels, (ii) demonstrate how gender biases can be abused to bypass cyberbullying detection models and (iii) show that allocational biases can be mitigated via training on our proposed dataset. Through our findings we highlight the importance of bias research moving beyond its current English-centrism.", "citations": 0}
{"title": "Polite Chatbot: A Text Style Transfer Application", "year": 2023, "authors": "Sourabrata Mukherjee, Vojtech Hudecek, Ondrej Dusek", "url": "https://api.semanticscholar.org/CorpusId:258378197", "relevance": 1, "abstract": "Generating polite responses is essential to build intelligent and engaging dialogue systems. However, this task is far from well-explored due to the difficulties of rendering a particular style in coherent responses, especially when parallel datasets for regular-to-polite pairs are usually unavailable.This paper proposes a polite chatbot that can produce responses that are polite and coherent to the given context.In this study, a politeness transfer model is first used to generate polite synthetic dialogue pairs of contexts and polite utterances. Then, these synthetic pairs are employed to train a dialogue model. Automatic and human evaluations demonstrate that our method outperforms baselines in producing polite dialogue responses while staying competitive in terms of coherent to the given context.", "citations": 19}
{"title": "What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive Content", "year": 2025, "authors": "Alfio Ferrara, Sergio Picascia, Laura Pinnavaia, Vojimir Ranitovic, Elisabetta Rocchetti, Alice Tuveri", "url": "https://api.semanticscholar.org/CorpusId:280401737", "relevance": 1, "abstract": "Proprietary Large Language Models (LLMs) have shown tendencies toward politeness, formality, and implicit content moderation. While previous research has primarily focused on explicitly training models to moderate and detoxify sensitive content, there has been limited exploration of whether LLMs implicitly sanitize language without explicit instructions. This study empirically analyzes the implicit moderation behavior of GPT-4o-mini when paraphrasing sensitive content and evaluates the extent of sensitivity shifts. Our experiments indicate that GPT-4o-mini systematically moderates content toward less sensitive classes, with substantial reductions in derogatory and taboo language. Also, we evaluate the zero-shot capabilities of LLMs in classifying sentence sensitivity, comparing their performances against traditional methods.", "citations": 0}
{"title": "Conversations: Love Them, Hate Them, Steer Them", "year": 2025, "authors": "Niranjan Chebrolu, G. Yeo, Kokil Jaidka", "url": "https://api.semanticscholar.org/CorpusId:278894827", "relevance": 1, "abstract": "Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable method for controlling specific emotional attributes in LLMs, contributing to developing more aligned and empathetic conversational AI.", "citations": 0}
{"title": "Can You be More Social? Injecting Politeness and Positivity into Task-Oriented Conversational Agents", "year": 2020, "authors": "Yi-Chia Wang, A. Papangelis, Runze Wang, Zhaleh Feizollahi, G\u00f6khan T\u00fcr, R. Kraut", "url": "https://api.semanticscholar.org/CorpusId:229923429", "relevance": 1, "abstract": "Goal-oriented conversational agents are becoming prevalent in our daily lives. For these systems to engage users and achieve their goals, they need to exhibit appropriate social behavior as well as provide informative replies that guide users through tasks. The first component of the research in this paper applies statistical modeling techniques to understand conversations between users and human agents for customer service. Analyses show that social language used by human agents is associated with greater users' responsiveness and task completion. The second component of the research is the construction of a conversational agent model capable of injecting social language into an agent's responses while still preserving content. The model uses a sequence-to-sequence deep learning architecture, extended with a social language understanding element. Evaluation in terms of content preservation and social language level using both human judgment and automatic linguistic measures shows that the model can generate responses that enable agents to address users' issues in a more socially appropriate way.", "citations": 9}
{"title": "MDCrow: Automating Molecular Dynamics Workflows with Large Language Models", "year": 2025, "authors": "Quintina Campbell, Sam Cox, Jorge Medina, Brittany Watterson, Andrew D. White", "url": "https://www.semanticscholar.org/paper/e244a179a5bbe39438c41beff07a0852d24ef8b5", "relevance": 1, "abstract": "Molecular dynamics (MD) simulations are essential for understanding biomolecular systems but remain challenging to automate. Recent advances in large language models (LLM) have demonstrated success in automating complex scientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an agentic LLM assistant capable of automating MD workflows. MDCrow uses chain-of-thought over 40 expert-designed tools for handling and processing files, setting up simulations, analyzing the simulation outputs, and retrieving relevant information from literature and databases. We assess MDCrow's performance across 25 tasks of varying required subtasks and difficulty, and we evaluate the agent's robustness to both difficulty and prompt style. \\texttt{gpt-4o} is able to complete complex tasks with low variance, followed closely by \\texttt{llama3-405b}, a compelling open-source model. While prompt style does not influence the best models' performance, it has significant effects on smaller models.", "citations": 16}
{"title": "Contextualized Embeddings for Enriching Linguistic Analyses on Politeness", "year": 2020, "authors": "Ahmad Aljanaideh, E. Fosler-Lussier, M. Marneffe", "url": "https://api.semanticscholar.org/CorpusId:227230317", "relevance": 1, "abstract": "Linguistic analyses in natural language processing (NLP) have often been performed around the static notion of words where the context (surrounding words) is not considered. For example, previous analyses on politeness have focused on comparing the use of static words such as personal pronouns across (im)polite requests without taking the context of those words into account. Current word embeddings in NLP do capture context and thus can be leveraged to enrich linguistic analyses. In this work, we introduce a model which leverages the pre-trained BERT model to cluster contextualized representations of a word based on (1) the context in which the word appears and (2) the labels of items the word occurs in. Using politeness as case study, this model is able to automatically discover interpretable, fine-grained context patterns of words, some of which align with existing theories on politeness. Our model further discovers novel finer-grained patterns associated with (im)polite language. For example, the word please can occur in impolite contexts that are predictable from BERT clustering. The approach proposed here is validated by showing that features based on fine-grained patterns inferred from the clustering improve over politeness-word baselines.", "citations": 4}
{"title": "Not Like Us, Hunty: Measuring Perceptions and Behavioral Effects of Minoritized Anthropomorphic Cues in LLMs", "year": 2025, "authors": "Jeffrey Basoah, Daniel Chechelnitsky, Tao Long, Katharina Reinecke, Chrysoula Zerva, Kaitlyn Zhou, M. D'iaz, Maarten Sap", "url": "https://api.semanticscholar.org/CorpusId:278481098", "relevance": 1, "abstract": "As large language models (LLMs) increasingly adapt and personalize to diverse sets of users, there is an increased risk of systems appropriating sociolects, i.e., language styles or dialects that are associated with specific minoritized lived experiences (e.g., African American English, Queer slang). In this work, we examine whether sociolect usage by a LLM agent affects user reliance on its outputs and user perception (satisfaction, frustration, trust, and social presence). We designed and conducted user studies where 498 African American English (AAE) speakers and 487 Queer slang speakers performed a set of question-answering tasks with LLM-based suggestions in either standard American English (SAE) or their self-identified sociolect. Our findings showed that sociolect usage by LLMs influenced both reliance and perceptions, though in some surprising ways. Results suggest that both AAE and Queer slang speakers relied more on the SAELM, and had more positive perceptions of the SAELM. Yet, only Queer slang speakers felt more social presence from the QSLM over the SAE one, whereas only AAE speakers preferred and trusted the SAELM over the AAE one. These findings emphasize the need to test for behavioral outcomes rather than simply assume that personalization would lead to a better and safer reliance outcome. They also highlight the nuanced dynamics of minoritized language in machine interactions, underscoring the need for LLMs to be carefully designed to respect cultural and linguistic boundaries while fostering genuine user engagement and trust.", "citations": 8}
{"title": "A computational approach to politeness with application to social factors", "year": 2013, "authors": "Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, J. Leskovec, Christopher Potts", "url": "https://www.semanticscholar.org/paper/2c1c912d3f52a504211170da758625212d317357", "relevance": 1, "abstract": "We propose a computational framework for identifying linguistic aspects of politeness. Our starting point is a new corpus of requests annotated for politeness, which we use to evaluate aspects of politeness theory and to uncover new interactions between politeness markers and context. These findings guide our construction of a classifier with domain-independent lexical and syntactic features operationalizing key components of politeness theory, such as indirection, deference, impersonalization and modality. Our classifier achieves close to human performance and is effective across domains. We use our framework to study the relationship between politeness and social power, showing that polite Wikipedia editors are more likely to achieve high status through elections, but, once elevated, they become less polite. We see a similar negative correlation between politeness and power on Stack Exchange, where users at the top of the reputation scale are less polite than those at the bottom. Finally, we apply our classifier to a preliminary analysis of politeness variation by gender and community.", "citations": 471}
{"title": "How to handle the truth: A model of politeness as strategic truth-stretching", "year": 2023, "authors": "Fausto Carcassi, Michael Franke", "url": "https://www.semanticscholar.org/paper/417cc94c13465c4b8f654e04974c8a58a3d3d1ad", "relevance": 1, "abstract": "", "citations": 3}
{"title": "A flexible defense against the winner\u2019s curse", "year": 2024, "authors": "Tijana Zrnic, William Fithian", "url": "https://api.semanticscholar.org/CorpusId:274306430", "relevance": 1, "abstract": "Across science and policy, decision-makers often need to draw conclusions about the best candidate among competing alternatives. For instance, researchers may seek to infer the effectiveness of the most successful treatment or determine which demographic group benefits most from a specific treatment. Similarly, in machine learning, practitioners are often interested in the population performance of the model that performs best empirically. However, cherry-picking the best candidate leads to the winner's curse: the observed performance for the winner is biased upwards, rendering conclusions based on standard measures of uncertainty invalid. We introduce the zoom correction, a novel approach for valid inference on the winner. Our method is flexible: it can be employed in both parametric and nonparametric settings, can handle arbitrary dependencies between candidates, and automatically adapts to the level of selection bias. The method easily extends to important related problems, such as inference on the top k winners, inference on the value and identity of the population winner, and inference on\"near-winners.\"", "citations": 3}
{"title": "Generating Politeness in Task Based Interaction: An Evaluation of the Effect of Linguistic Form and Culture", "year": 2007, "authors": "S. Gupta, M. Walker, D. Romano", "url": "https://api.semanticscholar.org/CorpusId:756810", "relevance": 1, "abstract": "Politeness is an integral part of human language variation, e.g. consider the difference in the pragmatic effect of realizing the same communicative goal with either \"Get me a glass of water mate!\" or \"I wonder if I could possibly have some water please?\" This paper presents POLLy (Politeness for Language Learning), a system which combines a natural language generator with an AI Planner to model Brown and Levinson's theory of politeness (BL (2) our indirect strategies which should be the politest forms, are seen as the rudest; and (3) English and Indian native speakers of English have different perceptions of the level of politeness needed to mitigate particular face threats.", "citations": 18}
{"title": "Polite Task-oriented Dialog Agents: To Generate or to Rewrite?", "year": 2022, "authors": "Diogo Gl\u00f3ria-Silva, David Semedo, Jo\u00e3o Magalh\u00e3es", "url": "https://www.semanticscholar.org/paper/4cb5bc2b8c1e4731f6b79041763f982ffe5aff84", "relevance": 1, "abstract": "For task-oriented dialog agents, the tone of voice mediates user-agent interactions, playing a central role in the flow of a conversation. Distinct from domain-agnostic politeness constructs, in specific domains such as online stores, booking platforms, and others, agents need to be capable of adopting highly specific vocabulary, with significant impact on lexical and grammatical aspects of utterances. Then, the challenge is on improving utterances\u2019 politeness while preserving the actual content, an utterly central requirement to achieve the task goal. In this paper, we conduct a novel assessment of politeness strategies for task-oriented dialog agents under a transfer learning scenario. We extend existing generative and rewriting politeness approaches, towards overcoming domain-shifting issues, and enabling the transfer of politeness patterns to a novel domain. Both automatic and human evaluation is conducted on customer-store interactions, over the fashion domain, from which contribute with insightful and experimentally supported lessons regarding the improvement of politeness in task-specific dialog agents.", "citations": 7}
{"title": "Characterizing Similarities and Divergences in Conversational Tones in Humans and LLMs by Sampling with People", "year": 2024, "authors": "Dun-Ming Huang, Pol van Rijn, Ilia Sucholutsky, Raja Marjieh, Nori Jacoby", "url": "https://api.semanticscholar.org/CorpusId:270285905", "relevance": 1, "abstract": "Conversational tones -- the manners and attitudes in which speakers communicate -- are essential to effective communication. Amidst the increasing popularization of Large Language Models (LLMs) over recent years, it becomes necessary to characterize the divergences in their conversational tones relative to humans. However, existing investigations of conversational modalities rely on pre-existing taxonomies or text corpora, which suffer from experimenter bias and may not be representative of real-world distributions for the studies' psycholinguistic domains. Inspired by methods from cognitive science, we propose an iterative method for simultaneously eliciting conversational tones and sentences, where participants alternate between two tasks: (1) one participant identifies the tone of a given sentence and (2) a different participant generates a sentence based on that tone. We run 100 iterations of this process with human participants and GPT-4, then obtain a dataset of sentences and frequent conversational tones. In an additional experiment, humans and GPT-4 annotated all sentences with all tones. With data from 1,339 human participants, 33,370 human judgments, and 29,900 GPT-4 queries, we show how our approach can be used to create an interpretable geometric representation of relations between conversational tones in humans and GPT-4. This work demonstrates how combining ideas from machine learning and cognitive science can address challenges in human-computer interactions.", "citations": 5}
{"title": "Studying Politeness across Cultures using English Twitter and Mandarin Weibo", "year": 2020, "authors": "Mingyang Li, Louis Hickman, L. Tay, L. Ungar, Sharath Chandra Guntuku", "url": "https://www.semanticscholar.org/paper/7a307b1ae75ef7875480e570cbce1c6516beded0", "relevance": 1, "abstract": "Modeling politeness across cultures helps to improve intercultural communication by uncovering what is considered appropriate and polite. We study the linguistic features associated with politeness across American English and Mandarin Chinese. First, we annotate 5,300 Twitter posts from the United States (US) and 5,300 Sina Weibo posts from China for politeness scores. Next, we develop an English and Chinese politeness feature set, 'PoliteLex'. Combining it with validated psycholinguistic dictionaries, we study the correlations between linguistic features and perceived politeness across cultures. We find that on Mandarin Weibo, future-focusing conversations, identifying with a group affiliation, and gratitude are considered more polite compared to English Twitter. Death-related taboo topics, use of pronouns (with the exception of honorifics), and informal language are associated with higher impoliteness on Mandarin Weibo than on English Twitter. Finally, we build language-based machine learning models to predict politeness with an F1 score of 0.886 on Mandarin Weibo and 0.774 on English Twitter.", "citations": 36}
{"title": "Controlling Politeness in Neural Machine Translation via Side Constraints", "year": 2016, "authors": "Rico Sennrich, B. Haddow, Alexandra Birch", "url": "https://www.semanticscholar.org/paper/bf0f141bae83bd6d5ca0c37839d53f0d06059b34", "relevance": 1, "abstract": "Many languages use honori\ufb01cs to express politeness, social distance, or the relative social status between the speaker and their ad-dressee(s). In machine translation from a language without honori\ufb01cs such as English, it is dif\ufb01cult to predict the appropriate honori\ufb01c, but users may want to control the level of politeness in the output. In this paper, we perform a pilot study to control honori\ufb01cs in neural machine translation (NMT) via side constraints , focusing on English \u2192 German. We show that by marking up the (English) source side of the training data with a feature that en-codes the use of honori\ufb01cs on the (German) target side, we can control the honori\ufb01cs produced at test time. Experiments show that the choice of honori\ufb01cs has a big impact on translation quality as measured by B LEU , and oracle experiments show that substantial im-provements are possible by constraining the translation to the desired level of politeness.", "citations": 332}
{"title": "The politeness Package: Detecting Politeness in Natural Language", "year": 2018, "authors": "Michael Yeomans, A. Kantor, D. Tingley", "url": "https://api.semanticscholar.org/CorpusId:68126412", "relevance": 1, "abstract": "This package provides tools to extract politeness markers in English natural language. It also allows researchers to easily visualize and quantify politeness between groups of documents. This package combines and extends prior research on the linguistic markers of politeness (Brown and Levinson, 1987; Danescu-Niculescu-Mizil et al., 2013; Voigt et al., 2017). We demonstrate two applications for detecting politeness in natural language during consequential social interactions\u2014 distributive negotiations, and speed dating.", "citations": 40}
{"title": "Is Politeness Better than Impoliteness? Comparisons of Robot's Encouragement Effects Toward Performance, Moods, and Propagation", "year": 2023, "authors": "Kana Higashino, Mitsuhiko Kimoto, T. Iio, K. Shimohara, M. Shiomi", "url": "https://api.semanticscholar.org/CorpusId:256773391", "relevance": 1, "abstract": "This study experimentally compared the effects of encouragement with polite/ impolite attitudes from a robot in a monotonous task from three viewpoints: performance, mood, and propagation. Experiment I investigated encouragement effects on performance and mood. The participants did a monotonous task during which a robot continuously provided polite, neutral, or impolite encouragement. Our experiment results showed that polite and impolite encouragement significantly improved performance more than neutral comments, although there was no significant difference between polite and impolite encouragement. In addition, impolite encouragement caused significantly more negative moods than polite encouragement. Experiment II determined whether the robot's encouragement influenced the participants' encouragement styles. The participants behaved similarly to the robot in Experiment I, i.e., they selected polite, neutral, and impolite encouragements by observing the progress of a monotonous task by a dummy participant. The experiment results, which showed that the robot's encouragement significantly influenced the participants' encouragement styles, suggest that polite encouragement is more advantageous than impolite encouragement.", "citations": 10}
{"title": "Modeling Politeness in Human-Robot Interaction", "year": 2024, "authors": "Eleonore Lumer", "url": "https://api.semanticscholar.org/CorpusId:268393427", "relevance": 1, "abstract": "Politeness is a linguistic phenomenon central to human communication. The many influences on the choice and interpretation of politeness in language make the phenomenon very complex to model and to account for in Human-Robot Interaction. I therefore question whether and how the implementation of such a social linguistic phenomenon in a robot is desirable. To find answers I conducted studies on politeness in Human-Robot Interaction from different perspectives: users' expectations, users' politeness towards a robot and users' perception of politeness use by a robot. In future research, based on and informed by my results, I aim to computationally model two politeness strategies and implement them in a Furhat robot.", "citations": 0}
{"title": "Courteously Yours: Inducing courteous behavior in Customer Care responses using Reinforced Pointer Generator Network", "year": 2019, "authors": "Hitesh Golchha, Mauajama Firdaus, Asif Ekbal, P. Bhattacharyya", "url": "https://www.semanticscholar.org/paper/ed1a894ad6652c81f4f96ae61860a94b6af7cc29", "relevance": 1, "abstract": "In this paper, we propose an effective deep learning framework for inducing courteous behavior in customer care responses. The interaction between a customer and the customer care representative contributes substantially to the overall customer experience. Thus it is imperative for customer care agents and chatbots engaging with humans to be personal, cordial and emphatic to ensure customer satisfaction and retention. Our system aims at automatically transforming neutral customer care responses into courteous replies. Along with stylistic transfer (of courtesy), our system ensures that responses are coherent with the conversation history, and generates courteous expressions consistent with the emotional state of the customer. Our technique is based on a reinforced pointer-generator model for the sequence to sequence task. The model is also conditioned on a hierarchically encoded and emotionally aware conversational context. We use real interactions on Twitter between customer care professionals and aggrieved customers to create a large conversational dataset having both forms of agent responses: \u2018generic\u2019 and \u2018courteous\u2019. We perform quantitative and qualitative analyses on established and task-specific metrics, both automatic and human evaluation based. Our evaluation shows that the proposed models can generate emotionally-appropriate courteous expressions while preserving the content. Experimental results also prove that our proposed approach performs better than the baseline models.", "citations": 25}
